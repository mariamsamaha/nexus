# ğŸŒŒ Nexus: Parallel Image Processing - Edge Detection

## Project Overview

**Nexus** is a scalable parallel and distributed system engineered for **high-performance edge detection** in images. The core goal of this project is to explore and implement various parallelism and distribution strategies to achieve measurable **speedup**, robust **scalability**, and effective **fault tolerance** in image processing tasks.

---

## ğŸ¯ Key Features and Goals

- **High-Performance Edge Detection:** Implementing established algorithms (e.g., Canny, Sobel) with a focus on speed.
- **Parallel Computing:** Utilizing multi-core architectures to process image data concurrently.
- **Distributed Scalability:** Spreading the workload across multiple machines/nodes for handling large datasets.
- **Fault Tolerance:** Designing the system to continue operation even if individual nodes or processes fail.
- **Performance Benchmarking:** Rigorous measurement and comparison of speedup across the different implementation phases.

---

## ğŸ—ºï¸ Project Phases

The development of Nexus is structured into three progressive phases, each building upon the previous one to introduce increasingly complex and powerful computing paradigms:

### Phase 1: Shared-Memory Parallelism (Single-Node Optimization) âš¡

- **Focus:** Implementing **threads** and/or **processes** (e.g., using OpenMP, pthreads, or language-native features) to leverage multi-core CPUs.
- **Goal:** Demonstrate initial, significant speedup compared to a sequential implementation on a single machine.
- **Implementation:** Parallelization of core image filtering and gradient computation steps.

### Phase 2: Distributed Computing (Multi-Node Scaling) ğŸŒ

- **Focus:** Moving to a multi-node environment using a message-passing interface (e.g., **MPI** or similar RPC framework).
- **Goal:** Achieve horizontal scalability by partitioning image data and processing it across a cluster of machines.
- **Implementation:** Development of communication protocols for data distribution, synchronization, and result aggregation.

### Phase 3: Big Data Processing & Fault Tolerance ğŸ›¡ï¸

- **Focus:** Integrating a **big-data framework** (e.g., Apache Spark, Hadoop MapReduce) to manage large-scale data and inherent fault tolerance.
- **Goal:** Process massive datasets (e.g., video streams, large image archives) efficiently while ensuring the system can recover from node failures without losing data.
- **Implementation:** Adapt the edge detection logic to run as a distributed job within the chosen framework, utilizing its built-in mechanisms for resilience and scheduling.

---

## ğŸ› ï¸ Technologies

---

## ğŸš€ Getting Started

Below is the minimal end-to-end flow: convert inputs as needed, run the Sobel edge detector, and (optionally) convert the result to PNG.

### 1) Dependencies

- C build tools (clang/gcc)
- Python 3
- Python: Pillow (for conversions)

Install Pillow:

```bash
python3 -m pip install --user Pillow
```

### 2) Build the sequential Sobel tool

The `sequential` binary reads any common image via `stb_image.h` (e.g., JPEG/PNG) and outputs a PGM edge map.

```bash
# Example (adjust if you have a different build setup)
clang -O2 -o sequential src/sequential.c -lm
```

### 3) Convert input (optional)

If you prefer/need to work with PGM explicitly, you can convert images first:

```bash
python3 convert_to_pgm.py data/dog.jpg data/dog.pgm
```

### 4) Run edge detection

Usage:

```bash
./sequential <input_image> <output_edges.pgm> [threshold]
```

- `input_image`: JPEG/PNG/PGM supported
- `output_edges.pgm`: binary PGM edge map
- `threshold` (optional): 0â€“255, default 100

Example:

```bash
./sequential data/dog.jpg data/dog_edges.pgm 100
```

### 5) Convert PGM edges to PNG (optional)

```bash
python3 convert_to_png.py data/dog_edges.pgm data/dog_edges.png
```

If you do not have Pillow, you can install it as above. Alternatively, install ImageMagick and use `convert` directly:

```bash
brew install imagemagick
convert data/dog_edges.pgm data/dog_edges.png
```

### Notes

- `stb_image.h` is a single-header loader included in `src/` that lets the C program read common formats without extra libraries. If you want to remove it, restrict inputs to PGM and run conversions beforehand.

---

# ğŸ§ª test.c â€” Memory Benchmark (Stride / Block Access)

This benchmark tests how memory access patterns affect performance, useful for analyzing cache locality and bandwidth during Phase 1 optimization.

---

## *Build*

```bash
# Linux / macOS
gcc -O3 -march=native -std=c11 -o memtest src/test.c -lm
# or
clang -O3 -march=native -std=c11 -o memtest src/test.c -lm
```
---

## *Usage*

```bash
./memtest <mode> <param> [iters=5] [array_MB=512]
```
Modes:
* stride <stride_in_elements> walks memory with a fixed stride (tests spatial locality).
Example:
```bash
./memtest stride 1 5 512
```
* block <block_size_in_elems> processes memory in 2D tiles (models tiling/local reuse).
Example:
```bash
./memtest block 64 5 512
```
Output: per-iteration time, accesses, bytes read, average access time (ns), checksum, and bandwidth (MB/s).

---

## *Collecting Hardware Counters (Linux)*
```bash
sudo perf stat -e cycles,instructions,cache-references,cache-misses,L1-dcache-loads,L1-dcache-load-misses \
-o perf_stride_1.txt ./memtest stride 1 5 512
```
Run a sweep for multiple strides:
```bash
for s in 1 2 4 8 16 32 64 128 256 512 1024; do
  echo "stride=$s"
  sudo perf stat -e cycles,instructions,cache-references,cache-misses,L1-dcache-loads,L1-dcache-load-misses \
    -o perf_stride_${s}.txt ./memtest stride $s 5 512
done
```
Tip: Pin to one core for reproducibility:
```bash
taskset -c 0 sudo perf stat -e ... ./memtest stride 1 5 512
```

---

# ğŸŒ Phase 2: Distributed Computing (MPI)

This phase implements the Sobel Edge Detector using **MPI (Message Passing Interface)** for distributed memory systems. It includes domain decomposition, halo exchange, and performance benchmarking.

## ğŸ“‚ Directory Structure

```
phase2/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ sobel_mbi.c            # Main MPI implementation
â”‚   â”œâ”€â”€ mpi_latency_bandwidth.c # Network microbenchmark
â”‚   â””â”€â”€ stb_image.h            # Image loading library
â”œâ”€â”€ run_scripts/
â”‚   â”œâ”€â”€ run_strong_scaling.sh  # Script for strong scaling test
â”‚   â”œâ”€â”€ run_weak_scaling.sh    # Script for weak scaling test
â”‚   â””â”€â”€ run_latency_bandwidth.sh # Script for latency/bandwidth test
â”œâ”€â”€ plots/
â”‚   â”œâ”€â”€ scaling_plot.py        # Generates scaling graphs
â”‚   â””â”€â”€ bandwidth_plot.py      # Generates latency/bandwidth graphs
â””â”€â”€ results/                   # Output CSVs and PNG plots
```

## ğŸ› ï¸ Compilation

You need an MPI implementation (e.g., OpenMPI, MPICH) installed.

### 1. Compile the Sobel Application
```bash
mpicc -O3 -std=c99 -o phase2/src/sobel_mbi phase2/src/sobel_mbi.c -lm
```

### 2. Compile the Microbenchmark
```bash
mpicc -O3 -std=c99 -o phase2/src/mpi_latency_bandwidth phase2/src/mpi_latency_bandwidth.c
```

## ğŸš€ Running Experiments

We provide automated scripts to run the benchmarks. Ensure you are in the project root directory.

### 1. Latency & Bandwidth
Measures point-to-point communication performance (ping-pong and streaming).
```bash
./phase2/run_scripts/run_latency_bandwidth.sh
```
*   **Output**: `phase2/results/latency_bandwidth/latency_bandwidth.csv`

### 2. Strong Scaling
Measures speedup with a fixed problem size (`dog.jpg`) as ranks increase (1, 2, 4, 8, 16).
```bash
./phase2/run_scripts/run_strong_scaling.sh
```
*   **Output**: `phase2/results/strong_scaling/strong_scaling.csv`

### 3. Weak Scaling
Measures efficiency with a fixed workload per rank (image size increases with ranks).
```bash
./phase2/run_scripts/run_weak_scaling.sh
```
*   **Output**: `phase2/results/weak_scaling/weak_scaling.csv`

## ğŸ“Š Generating Plots

Python scripts are provided to visualize the results. Requires `pandas` and `matplotlib`.

```bash
# Install dependencies (if needed)
pip install pandas matplotlib

# Generate Latency/Bandwidth Plot
python3 phase2/plots/bandwidth_plot.py

# Generate Scaling Plots (Speedup & Efficiency)
python3 phase2/plots/scaling_plot.py
```

Plots will be saved in the respective `phase2/results/` subdirectories.
